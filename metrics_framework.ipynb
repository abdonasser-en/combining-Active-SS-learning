{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nasserali/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from preprocess.dataset import get_MNIST,get_dataset,get_handler\n",
    "from models.model import Model\n",
    "from al_methods.least_confidence import LeastConfidence\n",
    "from al_methods.entropy_sampling import EntropySampling\n",
    "from al_methods.batch_BALD import BatchBALD\n",
    "from al_methods.core_set import CoreSet\n",
    "from ssl_methods.semi_fixmatch import fixmatch\n",
    "from ssl_methods.semi_flexmatch import flexmatch\n",
    "from ssl_methods.semi_pseudolabel import pseudolabel\n",
    "import models\n",
    "from torchvision import transforms\n",
    "from framework.framework2 import Framework2\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import al_methods\n",
    "import os\n",
    "import seaborn as sns \n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_pool = {'mnist':\n",
    "                { \n",
    "                 'n_class':10,\n",
    "                 'channels':1,\n",
    "                 'size': 28,\n",
    "                 'transform_tr': transforms.Compose([\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                 'transform_te': transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                 'loader_tr_args':{'batch_size': 128, 'num_workers': 8},\n",
    "                 'loader_te_args':{'batch_size': 1024, 'num_workers': 8},\n",
    "                 'normalize':{'mean': (0.1307,), 'std': (0.3081,)},\n",
    "                },\n",
    "\n",
    "            'svhn':\n",
    "                {\n",
    "                 'n_class':10,\n",
    "                'channels':3,\n",
    "                'size': 32,\n",
    "                'transform_tr': transforms.Compose([ \n",
    "                                    transforms.RandomCrop(size = 32, padding=4),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.4377, 0.4438, 0.4728), (0.1980, 0.2010, 0.1970))]),\n",
    "                 'transform_te': transforms.Compose([transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.4377, 0.4438, 0.4728), (0.1980, 0.2010, 0.1970))]),\n",
    "                 'loader_tr_args':{'batch_size': 128, 'num_workers': 8},\n",
    "                 'loader_te_args':{'batch_size': 1024, 'num_workers': 8},\n",
    "                 'normalize':{'mean': (0.4377, 0.4438, 0.4728), 'std': (0.1980, 0.2010, 0.1970)},\n",
    "                },\n",
    "            'cifar10':\n",
    "                {\n",
    "                 'n_class':10,\n",
    "                 'channels':3,\n",
    "                 'size': 32,\n",
    "                 'transform_tr': transforms.Compose([\n",
    "                                    transforms.RandomCrop(size = 32, padding=4),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))]),\n",
    "                 'transform_te': transforms.Compose([transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))]),\n",
    "                 'loader_tr_args':{'batch_size': 256, 'num_workers': 8},\n",
    "                 'loader_te_args':{'batch_size': 512, 'num_workers': 8},\n",
    "                 'normalize':{'mean': (0.4914, 0.4822, 0.4465), 'std': (0.2470, 0.2435, 0.2616)},\n",
    "                 },\n",
    "\n",
    "\n",
    "            'cifar100': \n",
    "               {\n",
    "                'n_class':100,\n",
    "                'channels':3,\n",
    "                'size': 32,\n",
    "                'transform_tr': transforms.Compose([\n",
    "                                transforms.RandomCrop(size = 32, padding=4),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))]),\n",
    "                'transform_te': transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))]),\n",
    "                'loader_tr_args':{'batch_size': 2048, 'num_workers': 4},\n",
    "                'loader_te_args':{'batch_size': 512, 'num_workers': 8},\n",
    "                'normalize':{'mean': (0.5071, 0.4867, 0.4408), 'std': (0.2675, 0.2565, 0.2761)},\n",
    "                }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the main \n",
    "X_tr, Y_tr, X_te, Y_te = get_dataset(\"Mnist\", \"./datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the main \n",
    "if type(X_tr) is list:\n",
    "    X_tr = np.array(X_tr)\n",
    "    Y_tr = torch.tensor(np.array(Y_tr))\n",
    "    X_te = np.array(X_te)\n",
    "    Y_te = torch.tensor(np.array(Y_te))\n",
    "\n",
    "if type(X_tr[0]) is not np.ndarray:\n",
    "    X_tr = X_tr.numpy()\n",
    "    X_te = X_te.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin with :  600  Number of round 5  How many to query 600\n"
     ]
    }
   ],
   "source": [
    "# in the main\n",
    "n_pool = len(Y_tr)\n",
    "n_test = len(Y_te)\n",
    "#in the main\n",
    "handler = get_handler(\"mnist\")\n",
    "# main or framewrok to see\n",
    "nEnd=6 # total number to query \n",
    "nQuery=1 # nombre of points to query in batch \n",
    "nStart=1 # nbre of points to start\n",
    "NUM_INIT_LB = int(nStart*n_pool/100)\n",
    "NUM_QUERY = int(nQuery*n_pool/100) if nStart!= 100 else 0\n",
    "NUM_ROUND = int((int(nEnd*n_pool/100) - NUM_INIT_LB)/ NUM_QUERY) if nStart!= 100 else 0\n",
    "if NUM_QUERY != 0:\n",
    "    if (int(nEnd*n_pool/100) - NUM_INIT_LB)% NUM_QUERY != 0:\n",
    "        NUM_ROUND += 1\n",
    "print(\"begin with : \",NUM_INIT_LB,\" Number of round\",NUM_ROUND,\" How many to query\",NUM_QUERY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the main file\n",
    "idxs_lb = np.zeros(n_pool, dtype=bool)\n",
    "idxs_lb\n",
    "# in the main file \n",
    "idxs_tmp = np.arange(n_pool)\n",
    "idxs_tmp\n",
    "np.random.shuffle(idxs_tmp)\n",
    "idxs_tmp\n",
    "# in the main file\n",
    "idxs_lb[idxs_tmp[:NUM_INIT_LB]] = True\n",
    "idxs_lb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self,n_class,img_size,channels,transform_tr,transform_te,loader_tr_args,loader_te_args,normalize):\n",
    "        self.n_class=n_class\n",
    "        self.img_size=img_size\n",
    "        self.channels=channels\n",
    "        self.transform_tr=transform_tr\n",
    "        self.transform_te=transform_te\n",
    "        self.loader_tr_args=loader_tr_args\n",
    "        self.loader_te_args=loader_te_args\n",
    "        self.normalize=normalize\n",
    "        self.dataset='mnist'\n",
    "        self.save_path='./save'\n",
    "        self.model='ResNet50'\n",
    "        self.lr=0.1\n",
    "        self.schedule = [20, 40]\n",
    "        self.momentum=0.9\n",
    "        self.gammas=[0.1,0.1]\n",
    "        self.framework='framwork1'\n",
    "        self.optimizer='SGD'\n",
    "        self.save_model=False\n",
    "        self.ALstrat='LeastConfidence'\n",
    "        self.SSLstrat='fixmatch'\n",
    "        self.n_epoch=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_args = args_pool[\"mnist\"]\n",
    "n_class = dataset_args['n_class']\n",
    "img_size = dataset_args['size']\n",
    "channels = dataset_args['channels']\n",
    "transform_tr = dataset_args['transform_tr']\n",
    "transform_te = dataset_args['transform_te']\n",
    "loader_tr_args = dataset_args['loader_tr_args']\n",
    "loader_te_args = dataset_args['loader_te_args']\n",
    "normalize = dataset_args['normalize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=Args(n_class,img_size,channels,transform_tr,transform_te,loader_tr_args,loader_te_args,normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1=models.__dict__[\"MobileNet\"]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.feature_extractor.conv1=torch.nn.Conv2d(args_pool['mnist']['channels'],16,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "model_1.discriminator.dis_fc2=torch.nn.Linear(in_features=50,out_features=args_pool['mnist']['n_class'],bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2=models.__dict__[\"ResNet50\"](n_class=args_pool['mnist']['n_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2.feature_extractor.conv1=torch.nn.Conv2d(args_pool['mnist']['channels'],16,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "# model_2.discriminator.dis_fc2=torch.nn.Linear(in_features=50,out_features=args_pool['mnist']['n_class'],bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_model_1= Framework2(X_tr, Y_tr, X_te, Y_te, idxs_lb, model_1, handler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (feature_extractor): resnet_fea(\n",
       "    (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): resnet_clf(\n",
       "    (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       "  (discriminator): resnet_dis(\n",
       "    (dis_fc1): Linear(in_features=512, out_features=50, bias=True)\n",
       "    (dis_fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "framework_model_1.model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 1 GPUs!\n",
      "[Batch=000] [Loss=2.33]\n",
      "\n",
      "==>>[2023-08-30 08:47:44] [Epoch=000/005] [framwork1(LeastConfidence+fixmatch) Need: 00:00:00] [LR=0.1000] [Best : Test Accuracy=0.00, Error=1.00]\n",
      "[Batch=000] [Loss=2.49]\n",
      "\n",
      "==>>[2023-08-30 08:47:45] [Epoch=001/005] [framwork1(LeastConfidence+fixmatch) Need: 00:00:11] [LR=0.1000] [Best : Test Accuracy=0.10, Error=0.90]\n",
      "[Batch=000] [Loss=1.75]\n",
      "\n",
      "==>>[2023-08-30 08:47:47] [Epoch=002/005] [framwork1(LeastConfidence+fixmatch) Need: 00:00:06] [LR=0.1000] [Best : Test Accuracy=0.10, Error=0.90]\n",
      "[Batch=000] [Loss=1.21]\n",
      "\n",
      "==>>[2023-08-30 08:47:48] [Epoch=003/005] [framwork1(LeastConfidence+fixmatch) Need: 00:00:03] [LR=0.1000] [Best : Test Accuracy=0.10, Error=0.90]\n",
      "[Batch=000] [Loss=0.96]\n",
      "\n",
      "==>>[2023-08-30 08:47:50] [Epoch=004/005] [framwork1(LeastConfidence+fixmatch) Need: 00:00:01] [LR=0.1000] [Best : Test Accuracy=0.15, Error=0.85]\n",
      "---- save figure the accuracy/loss curve of train/val into ./save/mnist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2801"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "framework_model_1.train(alpha=2e-3,n_epoch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# framework_model_2= Framework2(X_tr, Y_tr, X_te, Y_te, idxs_lb, model_2, handler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 1 GPUs!\n",
      "[Batch=000] [Loss=2.38]\n",
      "\n",
      "==>>[2023-08-30 08:47:52] [Epoch=000/005] [framwork1(LeastConfidence+fixmatch) Need: 00:00:00] [LR=0.1000] [Best : Test Accuracy=0.00, Error=1.00]\n",
      "[Batch=000] [Loss=2.98]\n",
      "\n",
      "==>>[2023-08-30 08:47:53] [Epoch=001/005] [framwork1(LeastConfidence+fixmatch) Need: 00:00:06] [LR=0.1000] [Best : Test Accuracy=0.28, Error=0.72]\n",
      "[Batch=000] [Loss=2.32]\n",
      "\n",
      "==>>[2023-08-30 08:47:55] [Epoch=002/005] [framwork1(LeastConfidence+fixmatch) Need: 00:00:04] [LR=0.1000] [Best : Test Accuracy=0.28, Error=0.72]\n",
      "[Batch=000] [Loss=2.24]\n",
      "\n",
      "==>>[2023-08-30 08:47:56] [Epoch=003/005] [framwork1(LeastConfidence+fixmatch) Need: 00:00:03] [LR=0.1000] [Best : Test Accuracy=0.28, Error=0.72]\n",
      "[Batch=000] [Loss=2.19]\n",
      "\n",
      "==>>[2023-08-30 08:47:58] [Epoch=004/005] [framwork1(LeastConfidence+fixmatch) Need: 00:00:01] [LR=0.1000] [Best : Test Accuracy=0.28, Error=0.72]\n",
      "---- save figure the accuracy/loss curve of train/val into ./save/mnist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2801"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "framework_model_1.train(alpha=2e-3,n_epoch=5,framework_model_2=True,model_2=framework_model_1.model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unlabeled data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices where the boolean matrix is False\n",
    "false_indices = np.argwhere(idxs_lb == False).squeeze()\n",
    "\n",
    "U_X_tr=X_tr[false_indices]\n",
    "U_Y_tr=Y_tr[false_indices]\n",
    "\n",
    "unlabeled_data=DataLoader(handler(U_X_tr[:NUM_QUERY],U_Y_tr[:NUM_QUERY],transform=transform_te),batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fr_1=framework_model_1.model.eval()\n",
    "predictions_model_1 = []\n",
    "with torch.no_grad():\n",
    "    for x, _,_ in unlabeled_data:\n",
    "        # Move the batch to the appropriate device (CPU or GPU)\n",
    "        x= x.to(framework_model_1.device)  # device could be 'cuda' or 'cpu'\n",
    "        # Forward pass to obtain predictions\n",
    "        batch_predictions,_= model_fr_1(x)\n",
    "        \n",
    "        # Append batch predictions to the list\n",
    "        predictions_model_1.append(batch_predictions.cpu().numpy())  # Move predictions to CPU and convert to numpy\n",
    "\n",
    "# Concatenate predictions from all batches\n",
    "predictions_model_1 = np.concatenate(predictions_model_1, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 5, 6, 5, 2, 6, 5, 1, 2, 5, 2, 5, 2, 1, 7, 2, 5, 6, 7, 2, 2,\n",
       "       4, 4, 2, 5, 5, 5, 5, 5, 5, 5, 6, 7, 2, 2, 2, 2, 5, 2, 1, 2, 7, 7,\n",
       "       7, 5, 5, 2, 5, 5, 5, 2, 7, 5, 7, 5, 5, 5, 5, 6, 2, 5, 2, 2, 2, 5,\n",
       "       2, 4, 2, 2, 2, 7, 1, 2, 5, 2, 2, 5, 6, 7, 5, 2, 2, 2, 7, 5, 5, 5,\n",
       "       2, 5, 2, 7, 5, 2, 5, 2, 7, 5, 1, 5, 7, 1, 7, 1, 2, 2, 2, 2, 2, 5,\n",
       "       5, 1, 2, 2, 5, 2, 2, 2, 5, 5, 2, 7, 1, 5, 2, 2, 6, 2, 5, 5, 2, 7,\n",
       "       5, 2, 5, 5, 2, 2, 5, 7, 2, 5, 5, 5, 5, 2, 2, 5, 2, 2, 6, 7, 7, 2,\n",
       "       2, 2, 7, 2, 5, 2, 5, 5, 2, 2, 2, 5, 5, 2, 5, 2, 7, 5, 1, 2, 7, 2,\n",
       "       2, 5, 2, 5, 5, 5, 0, 7, 2, 2, 5, 2, 2, 2, 2, 7, 2, 7, 7, 5, 5, 2,\n",
       "       6, 5, 2, 5, 6, 6, 2, 5, 6, 2, 2, 6, 7, 2, 7, 2, 2, 2, 2, 5, 5, 2,\n",
       "       2, 7, 2, 5, 5, 5, 5, 2, 7, 6, 2, 2, 2, 2, 5, 2, 2, 5, 5, 2, 2, 7,\n",
       "       5, 5, 5, 5, 6, 2, 5, 6, 2, 2, 5, 2, 6, 2, 7, 5, 2, 2, 2, 7, 5, 5,\n",
       "       2, 5, 5, 6, 1, 2, 5, 5, 2, 2, 1, 2, 2, 5, 2, 5, 2, 5, 7, 2, 2, 7,\n",
       "       2, 6, 2, 2, 2, 2, 5, 2, 2, 5, 2, 5, 7, 2, 2, 7, 7, 5, 7, 5, 6, 6,\n",
       "       2, 5, 5, 5, 6, 5, 2, 5, 5, 2, 2, 2, 5, 2, 2, 2, 5, 2, 2, 2, 5, 5,\n",
       "       5, 2, 5, 7, 5, 2, 5, 5, 5, 5, 5, 6, 5, 2, 2, 7, 5, 6, 5, 7, 5, 6,\n",
       "       5, 6, 6, 2, 2, 5, 5, 2, 5, 2, 6, 2, 5, 5, 7, 7, 5, 2, 5, 2, 2, 5,\n",
       "       2, 2, 2, 2, 3, 5, 5, 2, 5, 7, 6, 5, 2, 2, 5, 5, 6, 2, 5, 6, 6, 2,\n",
       "       2, 2, 5, 2, 5, 5, 6, 2, 1, 5, 2, 7, 7, 5, 2, 5, 6, 2, 5, 5, 5, 2,\n",
       "       7, 5, 5, 5, 5, 2, 5, 2, 2, 5, 5, 5, 2, 7, 2, 5, 5, 2, 7, 2, 6, 2,\n",
       "       5, 7, 6, 2, 2, 6, 2, 2, 2, 1, 6, 5, 5, 2, 5, 5, 2, 2, 2, 2, 2, 1,\n",
       "       5, 7, 2, 2, 5, 2, 2, 5, 4, 2, 2, 5, 5, 5, 2, 5, 5, 6, 5, 5, 5, 2,\n",
       "       2, 5, 5, 5, 2, 2, 5, 5, 5, 2, 2, 7, 5, 5, 5, 5, 7, 2, 6, 7, 5, 1,\n",
       "       7, 5, 5, 5, 2, 5, 5, 7, 2, 5, 5, 7, 5, 2, 5, 2, 2, 2, 5, 2, 2, 5,\n",
       "       5, 2, 2, 6, 5, 1, 2, 5, 5, 2, 5, 5, 2, 2, 2, 2, 5, 5, 2, 5, 5, 5,\n",
       "       6, 5, 5, 2, 7, 5, 7, 5, 2, 2, 5, 7, 5, 2, 5, 5, 6, 6, 5, 5, 5, 2,\n",
       "       2, 5, 5, 5, 5, 5, 2, 5, 7, 7, 5, 5, 2, 6, 6, 2, 5, 2, 2, 2, 7, 5,\n",
       "       5, 2, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions_model_1,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fr_2=framework_model_1.model_2.eval()\n",
    "predictions_model_2 = []\n",
    "with torch.no_grad():\n",
    "    for x, _,_ in unlabeled_data:\n",
    "        # Move the batch to the appropriate device (CPU or GPU)\n",
    "        x= x.to(framework_model_1.device)  # device could be 'cuda' or 'cpu'\n",
    "        # Forward pass to obtain predictions\n",
    "        batch_predictions,_= model_fr_2(x)\n",
    "        \n",
    "        # Append batch predictions to the list\n",
    "        predictions_model_2.append(batch_predictions.cpu().numpy())  # Move predictions to CPU and convert to numpy\n",
    "\n",
    "# Concatenate predictions from all batches\n",
    "predictions_model_2 = np.concatenate(predictions_model_2, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9, 6, 9, 9, 9, 9, 9, 9, 6, 9, 9, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 6, 9, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       5, 9, 9, 9, 5, 9, 9, 5, 5, 9, 9, 9, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 5, 9, 5, 9, 6, 9, 5, 9, 5, 9, 6, 9, 5, 9, 9, 9, 5, 9, 5, 9,\n",
       "       5, 9, 9, 9, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 5, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 5, 9, 5, 5, 6, 9, 9, 9, 9, 6, 9, 9, 9, 9, 9, 9,\n",
       "       5, 9, 9, 9, 9, 9, 9, 9, 6, 9, 9, 9, 9, 9, 9, 5, 9, 9, 9, 9, 9, 9,\n",
       "       5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       6, 9, 9, 9, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 5, 9,\n",
       "       9, 9, 9, 9, 6, 9, 5, 9, 9, 5, 5, 9, 9, 9, 9, 9, 5, 6, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 5, 5, 9, 9, 9, 9, 5, 9, 9, 6, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 5, 9, 9, 5, 9, 9, 9, 9, 9, 9, 9, 6, 5, 9, 9, 9, 9, 9, 9, 9,\n",
       "       5, 9, 9, 9, 9, 6, 9, 9, 9, 6, 6, 9, 9, 9, 9, 9, 5, 5, 9, 9, 9, 5,\n",
       "       6, 9, 9, 9, 9, 9, 9, 9, 6, 5, 9, 9, 9, 9, 9, 9, 9, 5, 9, 9, 9, 9,\n",
       "       9, 9, 9, 6, 9, 5, 9, 9, 9, 9, 9, 9, 9, 9, 5, 9, 9, 9, 5, 9, 5, 9,\n",
       "       9, 9, 9, 9, 6, 9, 9, 9, 9, 9, 9, 9, 9, 6, 9, 9, 9, 9, 9, 9, 9, 6,\n",
       "       5, 9, 9, 9, 5, 9, 9, 6, 9, 9, 6, 9, 9, 9, 9, 5, 6, 6, 9, 9, 5, 9,\n",
       "       9, 6, 9, 9, 9, 9, 9, 9, 5, 9, 6, 9, 9, 9, 5, 9, 9, 9, 9, 9, 9, 9,\n",
       "       5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 6, 9, 6, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 9, 5, 9, 9, 9, 9, 9, 9, 9, 9, 6, 5, 9, 6, 9, 5,\n",
       "       9, 9, 9, 6, 9, 9, 9, 9, 9, 9, 9, 9, 6, 5, 9, 9, 6, 9, 9, 5, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 6, 9, 9, 9, 9, 5, 9, 9,\n",
       "       9, 5, 6, 9, 6, 5, 9, 9, 6, 9, 9, 5, 9, 5, 9, 5, 9, 9, 6, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 5, 9, 9, 9, 9, 9, 5, 9, 5, 9, 5, 9, 9, 9, 9, 9, 9,\n",
       "       6, 9, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 6, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 5, 9, 9, 9, 9, 9, 9, 6, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 6, 9, 9, 9, 9, 9, 9, 6, 9, 9, 9, 5, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 6, 9])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions_model_2,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation coefficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "res_pearson=stats.pearsonr(np.argmax(predictions_model_1,axis=1),np.argmax(predictions_model_2,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpearMan correlation coeffiecient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.ones((100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[2]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.ones((100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_spearman=stats.spearmanr(np.argmax(predictions_model_1,axis=1),np.argmax(predictions_model_2,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.argmax(predictions_model_1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 5, 6, 5, 2, 6, 5, 1, 2, 5, 2, 5, 2, 1, 7, 2, 5, 6, 7, 2, 2,\n",
       "       4, 4, 2, 5, 5, 5, 5, 5, 5, 5, 6, 7, 2, 2, 2, 2, 5, 2, 1, 2, 7, 7,\n",
       "       7, 5, 5, 2, 5, 5, 5, 2, 7, 5, 7, 5, 5, 5, 5, 6, 2, 5, 2, 2, 2, 5,\n",
       "       2, 4, 2, 2, 2, 7, 1, 2, 5, 2, 2, 5, 6, 7, 5, 2, 2, 2, 7, 5, 5, 5,\n",
       "       2, 5, 2, 7, 5, 2, 5, 2, 7, 5, 1, 5, 7, 1, 7, 1, 2, 2, 2, 2, 2, 5,\n",
       "       5, 1, 2, 2, 5, 2, 2, 2, 5, 5, 2, 7, 1, 5, 2, 2, 6, 2, 5, 5, 2, 7,\n",
       "       5, 2, 5, 5, 2, 2, 5, 7, 2, 5, 5, 5, 5, 2, 2, 5, 2, 2, 6, 7, 7, 2,\n",
       "       2, 2, 7, 2, 5, 2, 5, 5, 2, 2, 2, 5, 5, 2, 5, 2, 7, 5, 1, 2, 7, 2,\n",
       "       2, 5, 2, 5, 5, 5, 0, 7, 2, 2, 5, 2, 2, 2, 2, 7, 2, 7, 7, 5, 5, 2,\n",
       "       6, 5, 2, 5, 6, 6, 2, 5, 6, 2, 2, 6, 7, 2, 7, 2, 2, 2, 2, 5, 5, 2,\n",
       "       2, 7, 2, 5, 5, 5, 5, 2, 7, 6, 2, 2, 2, 2, 5, 2, 2, 5, 5, 2, 2, 7,\n",
       "       5, 5, 5, 5, 6, 2, 5, 6, 2, 2, 5, 2, 6, 2, 7, 5, 2, 2, 2, 7, 5, 5,\n",
       "       2, 5, 5, 6, 1, 2, 5, 5, 2, 2, 1, 2, 2, 5, 2, 5, 2, 5, 7, 2, 2, 7,\n",
       "       2, 6, 2, 2, 2, 2, 5, 2, 2, 5, 2, 5, 7, 2, 2, 7, 7, 5, 7, 5, 6, 6,\n",
       "       2, 5, 5, 5, 6, 5, 2, 5, 5, 2, 2, 2, 5, 2, 2, 2, 5, 2, 2, 2, 5, 5,\n",
       "       5, 2, 5, 7, 5, 2, 5, 5, 5, 5, 5, 6, 5, 2, 2, 7, 5, 6, 5, 7, 5, 6,\n",
       "       5, 6, 6, 2, 2, 5, 5, 2, 5, 2, 6, 2, 5, 5, 7, 7, 5, 2, 5, 2, 2, 5,\n",
       "       2, 2, 2, 2, 3, 5, 5, 2, 5, 7, 6, 5, 2, 2, 5, 5, 6, 2, 5, 6, 6, 2,\n",
       "       2, 2, 5, 2, 5, 5, 6, 2, 1, 5, 2, 7, 7, 5, 2, 5, 6, 2, 5, 5, 5, 2,\n",
       "       7, 5, 5, 5, 5, 2, 5, 2, 2, 5, 5, 5, 2, 7, 2, 5, 5, 2, 7, 2, 6, 2,\n",
       "       5, 7, 6, 2, 2, 6, 2, 2, 2, 1, 6, 5, 5, 2, 5, 5, 2, 2, 2, 2, 2, 1,\n",
       "       5, 7, 2, 2, 5, 2, 2, 5, 4, 2, 2, 5, 5, 5, 2, 5, 5, 6, 5, 5, 5, 2,\n",
       "       2, 5, 5, 5, 2, 2, 5, 5, 5, 2, 2, 7, 5, 5, 5, 5, 7, 2, 6, 7, 5, 1,\n",
       "       7, 5, 5, 5, 2, 5, 5, 7, 2, 5, 5, 7, 5, 2, 5, 2, 2, 2, 5, 2, 2, 5,\n",
       "       5, 2, 2, 6, 5, 1, 2, 5, 5, 2, 5, 5, 2, 2, 2, 2, 5, 5, 2, 5, 5, 5,\n",
       "       6, 5, 5, 2, 7, 5, 7, 5, 2, 2, 5, 7, 5, 2, 5, 5, 6, 6, 5, 5, 5, 2,\n",
       "       2, 5, 5, 5, 5, 5, 2, 5, 7, 7, 5, 5, 2, 6, 6, 2, 5, 2, 2, 2, 7, 5,\n",
       "       5, 2, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.argmax(predictions_model_2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nasserali/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:697: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import cohen_kappa_score as am_capa\n",
    "\n",
    "# _cohen_kappa=am_capa(np.ones((100,)),np.ones((100,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b[:3]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _cohen_kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Strategy for active learning{args.ALstrat} and strategy for semi-supervised learning used {args.SSLstrat}')\n",
    "\n",
    "stratAl_model_1=LeastConfidence(framework_model_1.X_tr, framework_model_1.Y_tr, framework_model_1.X_te, framework_model_1.Y_te, framework_model_1.idxs_lb, framework_model_1.net, framework_model_1.handler, framework_model_1.args,framework_model_1.n_pool,framework_model_1.device)\n",
    "stratSSL_model_1=pseudolabel(framework_model_1.X_tr, framework_model_1.Y_tr, framework_model_1.X_te, framework_model_1.Y_te, framework_model_1.idxs_lb, framework_model_1.net, framework_model_1.handler, framework_model_1.args,framework_model_1.n_pool,framework_model_1.device,framework_model_1.predict,framework_model_1.g)\n",
    "\n",
    "Framework2.train(alpha,n_epochs)\n",
    "\n",
    "test_acc=framework_model_1.predict(framework_model_1.X_te,framework_model_1.Y_te)\n",
    "acc = np.zeros(NUM_ROUND+1)\n",
    "acc[0] = test_acc\n",
    "\n",
    "for rd in range(1, NUM_ROUND+1):\n",
    "    \n",
    "    if rd%2==0:\n",
    "        # Al_methods\n",
    "        print('Round {}/{}'.format(rd, NUM_ROUND), flush=True)\n",
    "        labeled = len(np.arange(framework_model_1.n_pool)[framework_model_1.idxs_lb])\n",
    "        if NUM_QUERY > int(nEnd*framework_model_1.n_pool/100) - labeled:\n",
    "            NUM_QUERY = int(nEnd*framework_model_1.n_pool/100) - labeled\n",
    "            \n",
    "        # query\n",
    "        ts = time.time()\n",
    "        output = stratAl.query(NUM_QUERY)\n",
    "        q_idxs = output\n",
    "        framework_model_1.idxs_lb[q_idxs] = True\n",
    "        te = time.time()\n",
    "        tp = te - ts\n",
    "        \n",
    "        # update\n",
    "        framework_model_1.update(framework_model_1.idxs_lb)\n",
    "        if hasattr(stratAl, 'train'):\n",
    "        \n",
    "            best_test_acc=stratAl.train(alpha=2e-3, n_epoch=10)\n",
    "        else: best_test_acc = framework_model_1.train(alpha=2e-3, n_epoch=10)\n",
    "\n",
    "        t_iter = time.time() - ts\n",
    "        \n",
    "        # round accuracy\n",
    "        # test_acc = strategy.predict(X_te, Y_te)\n",
    "        acc[rd] = best_test_acc\n",
    "    else:\n",
    "        #SSL methods\n",
    "        \n",
    "        print('Round {}/{}'.format(rd, NUM_ROUND), flush=True)\n",
    "        labeled = len(np.arange(framework_model_1.n_pool)[framework_model_1.idxs_lb])\n",
    "        if NUM_QUERY > int(nEnd*framework_model_1.n_pool/100) - labeled:\n",
    "            NUM_QUERY = int(nEnd*framework_model_1.n_pool/100) - labeled\n",
    "            \n",
    "        # query\n",
    "        ts = time.time()\n",
    "\n",
    "        output = stratSSL.query(NUM_QUERY)\n",
    "        q_idxs = output\n",
    "        framework_model_1.idxs_lb[q_idxs] = True\n",
    "        te = time.time()\n",
    "        tp = te - ts\n",
    "        \n",
    "        # update\n",
    "        framework_model_1.update(framework_model_1.idxs_lb)\n",
    "        best_test_acc = stratSSL.train(alpha=2e-3, n_epoch=10)\n",
    "\n",
    "        t_iter = time.time() - ts\n",
    "        \n",
    "        # round accuracy\n",
    "        # test_acc = strategy.predict(X_te, Y_te)\n",
    "        acc[rd] = best_test_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
